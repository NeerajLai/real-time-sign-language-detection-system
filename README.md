According to the World Federation of the Deaf, there are more than 300 distinct sign languages used by approximately 70 million deaf individuals worldwide (Murray, 2018). The recognition of sign language holds great potential in dismantling communication barriers faced by sign language users within society. Most communication technologies have primarily been designed to support spoken or written language, which often leaves out sign language users. While communication tools like Imo (Pagebites, 2018) and WhatsApp (Acton & Koum, 2009) have become integral parts of our daily lives, they present challenges for deaf individuals. The deaf community encounters various difficulties when using these technologies to communicate with the hearing majority. Consequently, sign language, characterized by structured hand gestures, visual motions, and signs, serves as a vital communication system for facilitating daily interactions among the deaf and speech-impaired population. Sign language involves the utilization of various body parts, including fingers, hands, arms, head, body, and facial expressions. It encompasses five key parameters: hand shape, palm orientation, movement, location, and expression/non-manual signals. For a sign word to be accurately conveyed, all five of these parameters must be executed correctly. Sign language is a form of communication that relies on hand and body gestures, along with facial expressions and body postures. It is primarily used by individuals who are deaf and mute. Various sign languages exist globally, such as British Sign Language (BSL), Indian Sign Language, and American Sign Language (ASL). It's important to note that BSL is not readily understandable to ASL users, and vice versa. The development of a functional sign language recognition system holds the potential to enable non-signing individuals to communicate with the deaf without the need for an interpreter. Such a system could also be employed to generate speech or text, promoting greater independence among the deaf community. Unfortunately, no system with these capabilities has been developed thus far. In this project, our goal is to create a system capable of accurately classifying sign language gestures. Sign language recognition involves the conversion of user-displayed signs and gestures into text, bridging the communication divide between those who cannot speak and the general population. To achieve this, image processing algorithms and neural networks are utilized to translate gestures into corresponding text based on training data. This allows raw images and videos to be transformed into understandable text. Individuals with speech impairments use hand signs and gestures for communication, which can be challenging for those without prior knowledge of sign language to comprehend. Hence, there is a pressing need for a system that recognizes various signs and gestures, effectively conveying information to the general population. Such a system acts as a bridge, facilitating communication between physically challenged individuals and those who are not. 
